# Проект асинхронного парсинга документации Python
## Основная цель проекта
### Изучение возможностей фреймворка Scrapy
## Описание проекта
### Выполняется парсинг данных со страницы с общей информацией о PEP (https://peps.python.org/), переход по ссылкам и сбор данных о каждом PEP. Парсер подготавливает данные и сохраняет их в два файла формата csv в папку results.
## Запуск проекта
>. Клонировать проект из репозитория

```git clone git@github.com:AnzorKhamukov/scrapy_parser_pep.git```

>. Создать, активировать виртуальное окружение и в него установить зависимости:

```python3 -m venv venv```
```source venv/bin/activate```
```pip install -r requirements.txt```

>. Запустить парсер из командной строки:

```scrapy crawl pep -o pep.csv```

## Результаты: 
### Результатом работы парсера будет создание двух файлов:
pep_ДатаВремя.csv - содержит список всех PEP (number, name, status);
status_summary_ДатаВремя.csv - содержит сводку по статусам PEP: сколько найдено документов в каждом статусе (Status, Quantity). В последней строке этого файла в колонке Total выводится общее количество всех документов.
